# Multimodal DeepFake Classification

## ğŸ“Œ Project Overview
This project focuses on **Fine-Grained Multimodal DeepFake Classification** using the **FakeAVCeleb** dataset. The model is designed to classify videos into four categories:
- **RealVideo-RealAudio**
- **RealVideo-FakeAudio**
- **FakeVideo-RealAudio**
- **FakeVideo-FakeAudio**

The classification is achieved by processing both **video** (frames) and **audio** (waveforms) using deep learning architectures.

---

## ğŸ“‚ Directory Structure
```
FakeAVCeleb/
â”œâ”€â”€ dataset/
â”‚   â”œâ”€â”€ dataset.py  # Dataset processing class
â”‚   â”œâ”€â”€ audio_extraction.py  # Extracts audio from video
â”‚   â”œâ”€â”€ video_frame_extraction.py  # Extracts frames from video
â”‚
â”œâ”€â”€ network/
â”‚   â”œâ”€â”€ audio_processing_model.py  # Processes audio input
â”‚   â”œâ”€â”€ video_processing_model.py  # Processes video input
â”‚   â”œâ”€â”€ graph_video_audio_model.py  # Main model integrating video & audio features
â”‚
â”œâ”€â”€ summary/
â”‚   â”œâ”€â”€ weight/  # Stores trained model checkpoints
â”‚   â”œâ”€â”€ training_logs/  # Stores logs & evaluation reports
â”‚
â”œâ”€â”€ train.py  # Main training script
â”œâ”€â”€ steps.py  # Script to test single video/audio processing
â”œâ”€â”€ requirements.txt  # Python dependencies
â”œâ”€â”€ README.md  # Project documentation
```

---

## ğŸ“¦ Dependencies
Install required packages using:
```bash
pip install -r requirements.txt
```

Ensure you have **PyTorch**, **MoviePy**, **TorchAudio**, and **OpenCV** installed.

---

## ğŸš€ Training the Model
To train the model, run:
```bash
python train.py
```
This will:
- Load the dataset.
- Train the model using a **Graph Attention Network (GAT)**.
- Save checkpoints every 2 epochs.
- Log accuracy, loss, and evaluation metrics.

---

## ğŸ“Š Model Evaluation & Results
After training, the model is evaluated on the **test dataset**.

### âœ… **Final Training Metrics**
```
Epoch [15/15] - Loss: 0.1584, Train Acc: 93.78%
```

### ğŸ” **Classification Report**
| Class                      | Precision | Recall  | F1-score | Support |
|----------------------------|-----------|---------|----------|---------|
| RealVideo-RealAudio        | 0.897988  | 0.99590 | 0.92775  | 2000    |
| RealVideo-FakeAudio        | 0.865083  | 0.98741 | 0.92270  | 2000    |
| FakeVideo-RealAudio        | 0.956383  | 0.98399 | 0.97239  | 2000    |
| FakeVideo-FakeAudio        | 0.970878  | 0.97962 | 0.97522  | 2000    |
| **Accuracy**               |           | 0.95237 |          | 8000    |
| **Macro Avg**              | 0.922083  | 0.95237 | 0.94926  | 8000    |
| **Weighted Avg**           | 0.922083  | 0.95237 | 0.94926  | 8000    |

### ğŸ“‰ **Confusion Matrix**
![Confusion Matrix](summary/weight/confusion_matrix.png)

### ğŸ“œ **Logs & Reports**
- [Classification Report](summary/weight/classification_report.csv)
- [Training Loss & Accuracy Plots](summary/weight/training_plot.png)

---

## ğŸ’¡ Future Work
- Implementing **Larger Transformer-Based Models** (e.g., ViT, Wav2Vec).
- Using **Pretrained Video Models** (e.g., SlowFast, TimeSformer).
- Improving **Cross-Modal Attention** mechanisms.

---

## ğŸ¤ Contributing
Feel free to raise issues or contribute via pull requests.

---

## ğŸ“œ License
MIT License Â© 2025
